# ğŸ§  NLP Pipeline for Full-Text Screening Using LLMs and Embeddings

This repository implements a comprehensive pipeline to **automate the full-text screening** of scientific literature, particularly research papers in PDF format. It combines **OpenAI embeddings**, **LLM-based validation (GPT-4.1-mini)**, and optional **BioBERT fine-tuning** to assess whether text segments satisfy **user-defined inclusion criteria**. The output includes **annotated PDFs** with highlighted sections and justifications for each inclusion.

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ main.py                       # Entry point for the entire pipeline
â”œâ”€â”€ config.py                     # Inclusion criteria, colors, thresholds, API keys
â”œâ”€â”€ models/
â”‚   â””â”€â”€ biobert_trainer.py        # Optional: Fine-tune BioBERT on labeled data
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ check_chunk_llm.py        # LLM-based verification of semantic matches
â”‚   â”œâ”€â”€ embedding.py              # OpenAI embedding generator with caching
â”‚   â”œâ”€â”€ pdf_highlighter.py        # Applies highlights and tooltips to matched PDF text
â”‚   â”œâ”€â”€ pdf_parser.py             # Extracts sentence-based text chunks from PDFs
â”‚   â””â”€â”€ similarity.py             # Computes cosine similarity and filters matches
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ papers/                   # Input folder: drop PDFs to be screened here
â”‚   â””â”€â”€ output/                   # Output folder: highlighted and annotated PDFs
â””â”€â”€ .gitignore
```

---

## ğŸ§  What This Pipeline Does

### Problem
In systematic reviews or screening tasks, full-text PDF review is labor-intensive and subjective. This project automates the screening pipeline with semantic and logical checks using modern NLP tools.

### Solution Workflow
1. ğŸ“„ Input PDFs are chunked into overlapping sentences.
2. ğŸ”¢ Each chunk is embedded using OpenAIâ€™s `text-embedding-3-large`.
3. ğŸ§  The chunk is semantically compared to pre-embedded inclusion criteria.
4. âœ… Chunks with high similarity are verified using GPT-4.1-mini for human-like judgment.
5. ğŸ–ï¸ Validated chunks are annotated in the PDF with highlights and reasoning tooltips.

---

## ğŸ—ï¸ Pipeline Architecture

```mermaid
flowchart TD
    A[ğŸ“„ Input PDFs (data/papers)] --> B[ğŸ§© Sentence-Based Chunking]
    B --> C[ğŸ”¢ Embedding Generation<br/>(text-embedding-3-large)]
    C --> D[ğŸ“ Cosine Similarity Scoring<br/>against Inclusion Criteria]
    D -->|â‰¥ Threshold| E[ğŸ¤– LLM Verification (GPT-4.1-mini)]
    E --> F[ğŸ§  Save Validated Chunks with LLM Reasoning]
    F --> G[ğŸ–ï¸ Highlight Matches in PDF]
    G --> H[ğŸ“‚ Output PDFs (data/output)]
```

---

## ğŸ” Inclusion Criteria Explained

All inclusion criteria are defined in `config.py`. Each one contains:
- A **descriptive paragraph** used for semantic matching.
- A **label** (e.g., "Population", "Intervention").
- A **color code** used for highlighting.

These criteria are first embedded using OpenAI and then used to compare against paper chunks.

---

## ğŸ§ª How Matching Works (Deep Dive)

1. **Chunking**:
   - PDFs are read page-by-page using `PyMuPDF`.
   - Each page is split into sliding windows of 3â€“4 sentences with overlaps to ensure context continuity.
   - Each chunk is associated with its originating page number.

2. **Embedding**:
   - Both inclusion criteria and chunks are embedded using `text-embedding-3-large`.
   - Each chunk becomes a dense vector in semantic space.

3. **Similarity Scoring**:
   - Cosine similarity is calculated between each chunk vector and every criterion vector.
   - The chunk is assigned the criterion with the highest similarity.
   - Only chunks with similarity above a threshold (e.g., 0.5) are retained.

4. **LLM Verification**:
   - GPT-4.1-mini is queried via LangChain with:
     - The chunkâ€™s text
     - The matched criterion label
     - The full inclusion criterion description
   - The model answers:
     - YES or NO
     - An explanation (1â€“2 sentences)

5. **Annotation**:
   - Matched and verified chunks are saved with:
     - Criterion label and ID
     - Page number
     - GPT explanation
   - The original PDF is re-opened, and the matched text is highlighted.
   - Hover annotations in the PDF show the modelâ€™s reasoning.

6. **Output**:
   - Annotated PDFs are stored in `data/output/` for human review and traceability.

---

## âš™ï¸ Configuration

You can modify the following in `config.py`:
- `INCLUSION_CRITERIA`: Paragraph descriptions of each criterion.
- `SIMILARITY_THRESHOLD`: Filter threshold for cosine similarity.
- `SENTENCES_PER_CHUNK`: Controls chunk granularity.
- `CRITERIA_COLORS`: Colors for each criterionâ€™s highlight.
- `LLM_MODEL`, `OPENAI_MODEL`: Choose models to use.

---

## ğŸš€ Getting Started

### Step 1: Install Requirements

```bash
pip install -r requirements.txt
```

### Step 2: Set Up API Key

Create a `.env` file with:

```
OPENAI_API_KEY=your-key-here
```

### Step 3: Add PDFs

Drop PDFs into the input folder:

```
data/papers/
```

### Step 4: Run the Pipeline

```bash
python main.py
```

### Step 5: Review Annotated PDFs

Output files with highlights and LLM comments will be saved in:

```
data/output/
```

---

## ğŸ”¬ Optional: Train BioBERT on Labeled Data

If you have labeled inclusion/exclusion data:

```python
from models.biobert_trainer import train_biobert

train_biobert([
    {"text": "This is an NCD simulation model using burden-of-disease.", "label": 3},
    ...
])
```

This uses HuggingFaceâ€™s Trainer API for supervised fine-tuning.

---

## ğŸ’¡ Use Cases

- Systematic review support
- Automated inclusion/exclusion filtering
- Transparent evidence triage in health modeling
- Semantic filtering in scientific NLP pipelines

---

## ğŸ“œ License

This project is released under the MIT License.

---

## ğŸ™ Acknowledgments

- [OpenAI](https://openai.com/)
- [LangChain](https://www.langchain.com/)
- [HuggingFace](https://huggingface.co/)
- [PyMuPDF](https://pymupdf.readthedocs.io/)